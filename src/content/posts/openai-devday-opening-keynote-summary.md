---
title: "OpenAI DevDay Opening Keynote Summary"
description: "A summary of OpenAI's first DevDay Opening Keynote"
type: "AI"
publishDate: 2023-11-07
---

For those of you who prefer to watch a video I made on my channel regarding this subject:

<iframe class="mx-auto" width="560" height="315" src="https://www.youtube.com/embed/tlt-00imiac" title="OpenAI DevDay Opening Keynote Full Summary In 6 Minutes" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>

### GPT-4 Turbo

#### Context Length

While GPT-4 supports up to 8k and in some cases up to 32k tokens of context, GPT-4 Turbo supports up to 128k tokens of context, which just to get into perspective is equal to 300 pages of a standard book.

#### More Control

Noticing developers need more control over the model's responses and outputs, OpenAI decided to add the following features:

1. JSON load - ensures the model responds with a valid JSON with a click of a button.
2. Better at following instructions - you can now call many functions at once.
3. Reproducible outputs - you can pass the model a seed parameter making it return consistent outputs.
4. Log probabilities (coming out in a couple of weeks) - shows the most likely output tokens generated by GPT-4 Turbo and GPT-3.5 Turbo.

#### Better Knowledge

- You can now bring knowledge from outside documents and databases into whatever you are building.
- GPT Turbo will have knowledge of the world up to April 2023 and Sam Altman mentions that they will improve that over time.

#### New modalities

Things that are going into the API:

- DALLEÂ·3
- GPT-4 Turbo with Vision
- TTS

OpenAI's safety systems will help developers protect their applications against misuse.

Sam also mentions that GPT-4 Turbo can now accept images as inputs via the API, can generate captions, classifications, and analysis.

##### 'Images As Inputs' Example

Be My Eyes is an app that helps people who are blind or have low vision with their daily tasks like identifying products in front of them.

##### New Text-To-Speech Model

The new model will be able to generate natural-sounding audio from text in the API with six preset voices to choose from.

Sam adds that Whisper V3 (the next version of OpenAI's speech recognition model) comes out today and will be available soon on their API.

#### Customization

##### Fine-Tuning

For those who don't know, fine-tuning is great for adapting our models to achieve better performance in a wide variety of applications with a relatively small amount of data.

Since fine-tuning has been working really well for GPT-3.5, OpenAI decided to expand GPT-3.5 to the 16k version of the model.

##### New Program: Custom Models

For companies who join the program, OpenAIs researchers will work closely to help them make a great custom model, especially for them, and their use case.

#### Higher rate limits

OpenAI is doubling the tokens per minute for all of their established GPT-4 customers. Also, we will be able to request changes to further rate limits and quotas directly in their API account settings

#### Copyright Shield

OpenAI will step in, defend their customers, and pay the costs incurred in case their customers face legal claims around copyright
invasion. This place to both ChatGPT Enterprise and the API.

Sam also highlights that OpenAI do not train on our data from the API, or ChatGPT Enterprise, **ever**.

### Lower Prices

#### GPT-4 Turbo vs GPT-4

GPT-4 Turbo input tokens are **3x** cheaper than GPT-4 at $0.01 and output tokens are **2x** cheaper at $0.03. This means GPT-4 Turbo is overall **2.75x** cheaper than GPT-4.

#### GPT-3.5 Turbo 16K Model

GPT-3.5 Turbo input tokens are **3x** cheaper than the previous 16K model at $0.001 and output tokens are **2x** cheaper at $0.002.

#### Fine-tuned GPT-3.5 Turbo 4K Model

Fine-tuned GPT-3.5 Turbo 4K model input tokens are reduced by **4x** at $0.003 and output tokens are **2.7x** cheaper at $0.006.

### What's Next?

After prioritizing price first, OpenAI decided that from now on they will work on speed. Sam also mentions that soon we will be able to notice GPT-4 Turbo becoming a lot faster.

### ChatGPT Improvements

- ChatGPT now uses GPT-4 Turbo.
- No more annoying model picker, from now on ChatGPT will just know what to use and when you need it.

### GPTs

GPTs are custom versions of ChatGPT for a specific purpose. We will be able to create a customized version of ChatGPT, for almost anything, with
instructions, expanded knowledge, and actions, and then publish it for others to use.

#### Developing a GPT

OpenAI has made it so that we will be able to program the GPT just by having a conversation. They believe that natural language is going to be a big part of how people use computers in the near future.

#### Availability

We will be able to select who can use our GPT, from one of the following options:

- Private - only you
- Public - everyone
- Enterprise - your company (for ChatGPT enterprise only)

#### GPT Store

Later this month OpenAI will launch a store where you can publish your GPT, and they are going to pay people who build the most useful and the
most used GPTs a portion of their revenue.

### Assistants API

An API that will help building agent-like experiences like Shopify Sidekick or Discord's Clyde.

#### Features:

- Threading - persist threads that will help deal with long conversation history.
- Retrieval - built-in retrieval.
- Code Interpreter - a working Python interpreter in a sandbox environment.
- Improved Function Calling - lets the user call many functions at once.
  (Raman shows demo)
