---
title: "Software Engineer Learning Machine Learning - Day 3"
description: "This article focuses on Gradient Descent"
type: "AI"
publishDate: 2023-11-28
---

**_Disclaimer:_** If you haven't read/watched [day 2](https://excali-blog.vercel.app/posts/software-engineer-learning-machine-learning/2), I recommend you do that before reading this article.

For those of you who prefer to watch a video I made on my channel regarding my journey:

<iframe class="mx-auto" width="560" height="315" src="https://www.youtube.com/embed/I0imzb7fwqg" title="Software Engineer Learning Machine Learning Day 3" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>

So we have our hypothesis (hθ(x) = θ₀ + θ₁x) and we have a way of measuring how well it fits into the data (cost function). Our goal is to minimize the cost function, that way we know our hypothesis makes good guesses. That's where gradient descent comes in.

### Gradient Descent

Gradient Descent is an optimization algorithm for finding a local minimum of a differentiable function or in simple terms it helps us improve our hypothesis by minimizing the cost function.

A simple way to see the value of the cost function for each pair of θ₀, θ₁
would be putting θ₀ on the x-axis, θ₁ on the y-axis and the cost function on the vertical z-axis.

![Gradient Descent Graph Clear](/images/posts/software-engineer-learning-machine-learning/3/gradient-descent-graph-clear.PNG)

Remember, our goal is to minimize the cost function -> get to the very bottom of the pits in our graph.

Starting from arbitrarily θ₀ and θ₁, the way to do so is by checking the slope of the landscape at your current location and using that to walk downhill from your starting point until you reach the bottom.

![Gradient Descent Graph With Steps](/images/posts/software-engineer-learning-machine-learning/3/gradient-descent-graph-with-steps.PNG)

Your step size is affected by 2 things:

1. The slope of the landscape - the steeper the landscape is, the bigger your step.
2. The learning rate (α) - Too big and you might overshoot the minimum; too small, and it'll take forever.

\*α is the parameter in which we multiply the partial derivative of the cost function for updating both θ₀, θ₁.

#### The Bad News

You might have noticed that our starting point can change the final θ₀ and θ₁ we reach at the end of the gradient descent algorithm and that's true.

Starting from this position:
![Gradient Descent Graph Different Start](/images/posts/software-engineer-learning-machine-learning/3/gradient-descent-graph-different-start.PNG)

We might and up here:
![Gradient Descent Graph Different End](/images/posts/software-engineer-learning-machine-learning/3/gradient-descent-graph-different-end.PNG)

Which is obviously not: "the very bottom of the pits in our graph".

#### The Good News

For linear regression, there's only one lowest point (global minimum) on this landscape.

![Gradient Descent Graph Linear Regression](/images/posts/software-engineer-learning-machine-learning/3/gradient-descent-graph-linear-regression.png)

Gradient Descent will get you there, doesn't matter your starting point, assuming you don't take steps that are too big.
