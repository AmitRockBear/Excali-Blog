---
title: "Software Engineer Learning Machine Learning - Day 4"
description: "This article focuses on Multivariate Linear Regression and Gradient Descent"
type: "AI"
publishDate: 2023-11-29
---

**_Disclaimer:_** If you haven't read/watched [day 3](https://excali-blog.vercel.app/posts/software-engineer-learning-machine-learning/3), I recommend you do that before reading this article.

For those of you who prefer to watch a video I made on my channel regarding my journey:

<iframe class="mx-auto" width="560" height="315" src="https://www.youtube.com/embed/1nOj8jSfG0U" title="Software Engineer Learning Machine Learning Day 2" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>

While predicting the price of a house from its size is very nice, it does not describe what happens in our world. For example, the price of a house is also affected by the number of floors. That's what Multivariate Linear Regression comes to solve.

### Multivariate Linear Regression

Multivariate linear regression to put it simply, is linear regression with multiple variables.

Instead of having the hypothesis hθ(x) = θ₀ + θ₁x, we will now have hθ(x) = θ₀ + θ₁x₁ + ... + θₙxₙ.

Going back to the "housing price" example, for n=2:

- θ₀ - represents the basic price of a house.
- θ₁ - represents the price per square meter.
- θ₂ - represents the price per floor.

Also for those of you who know matrix multiplication, our hypothesis can be represented as follows, with x₀ being 1:
![Hypothesis As Matrix Multiplication](/images/posts/software-engineer-learning-machine-learning/4/hypothesis-as-matrix-multiplication.PNG)

#### Gradient Descent for Multiple Variables

The gradient descent equation itself is generally the same form; we just have to repeat it for our 'n' features:

![Gradient Descent for Multiple Variables](/images/posts/software-engineer-learning-machine-learning/4/grandient-descent-for-multiple-variables.PNG)

\* One thing to notice is that all variables θ₀,..., θₙ have to be updated simultaneously, just like we did before with one variable.

#### Feature Scaling And Mean Normalization

Going back to the "housing price" example, something you might have noticed is that while the size of a house in feet^2 is usually a number with 3 digits or more, the number of floors in a private house does not go beyond a single digit number (well at least for most of the houses). It turns out that the difference in the scale of the input values may slow gradient descent. That is where "feature scaling" and "mean normalization" come in.

- Feature scaling - adjusting the input values so they're all on a similar scale, like dividing them by the range maximum so they are all between 0 and 1. For example, if private houses have 9 floors at max:

![Feature Scaling Example](/images/posts/software-engineer-learning-machine-learning/4/feature-scaling-example.PNG)

- Mean normalization - making the average value for each input roughly zero by subtracting the average from each value. For example, if the housing prices are in the range of 100 to 2000 and have a mean value of 1000:

![Mean normalization Example](/images/posts/software-engineer-learning-machine-learning/4/mean-normalization-example.PNG)
